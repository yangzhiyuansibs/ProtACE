{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cuda\n",
      "Epoch 1/100, Loss: 0.1424, Acc: 0.5331, MCC: 0.0015, Sensitivity: 0.6427, Specificity: 0.3587\n",
      "Epoch 2/100, Loss: 0.1200, Acc: 0.5539, MCC: 0.0567, Sensitivity: 0.6407, Specificity: 0.4159\n",
      "Epoch 3/100, Loss: 0.1140, Acc: 0.6091, MCC: 0.1662, Sensitivity: 0.7006, Specificity: 0.4635\n",
      "Epoch 4/100, Loss: 0.1018, Acc: 0.6091, MCC: 0.1671, Sensitivity: 0.6986, Specificity: 0.4667\n",
      "Epoch 5/100, Loss: 0.0995, Acc: 0.6360, MCC: 0.2255, Sensitivity: 0.7186, Specificity: 0.5048\n",
      "Epoch 6/100, Loss: 0.0946, Acc: 0.6507, MCC: 0.2628, Sensitivity: 0.7166, Specificity: 0.5460\n",
      "Epoch 7/100, Loss: 0.0960, Acc: 0.6434, MCC: 0.2464, Sensitivity: 0.7126, Specificity: 0.5333\n",
      "Epoch 8/100, Loss: 0.0937, Acc: 0.6593, MCC: 0.2763, Sensitivity: 0.7345, Specificity: 0.5397\n",
      "Epoch 9/100, Loss: 0.0904, Acc: 0.6618, MCC: 0.2881, Sensitivity: 0.7206, Specificity: 0.5683\n",
      "Epoch 10/100, Loss: 0.0854, Acc: 0.6777, MCC: 0.3205, Sensitivity: 0.7365, Specificity: 0.5841\n",
      "Epoch 11/100, Loss: 0.0779, Acc: 0.6912, MCC: 0.3583, Sensitivity: 0.7246, Specificity: 0.6381\n",
      "Epoch 12/100, Loss: 0.0721, Acc: 0.6985, MCC: 0.3703, Sensitivity: 0.7385, Specificity: 0.6349\n",
      "Epoch 13/100, Loss: 0.0772, Acc: 0.6912, MCC: 0.3549, Sensitivity: 0.7325, Specificity: 0.6254\n",
      "Epoch 14/100, Loss: 0.0748, Acc: 0.7341, MCC: 0.4457, Sensitivity: 0.7645, Specificity: 0.6857\n",
      "Epoch 15/100, Loss: 0.0733, Acc: 0.6924, MCC: 0.3630, Sensitivity: 0.7206, Specificity: 0.6476\n",
      "Epoch 16/100, Loss: 0.0681, Acc: 0.7108, MCC: 0.4087, Sensitivity: 0.7186, Specificity: 0.6984\n",
      "Epoch 17/100, Loss: 0.0645, Acc: 0.7463, MCC: 0.4692, Sensitivity: 0.7804, Specificity: 0.6921\n",
      "Epoch 18/100, Loss: 0.0651, Acc: 0.7451, MCC: 0.4754, Sensitivity: 0.7565, Specificity: 0.7270\n",
      "Epoch 19/100, Loss: 0.0625, Acc: 0.7488, MCC: 0.4851, Sensitivity: 0.7545, Specificity: 0.7397\n",
      "Epoch 20/100, Loss: 0.0636, Acc: 0.7574, MCC: 0.4977, Sensitivity: 0.7745, Specificity: 0.7302\n",
      "Epoch 21/100, Loss: 0.0618, Acc: 0.7537, MCC: 0.4944, Sensitivity: 0.7605, Specificity: 0.7429\n",
      "Epoch 22/100, Loss: 0.0606, Acc: 0.7451, MCC: 0.4788, Sensitivity: 0.7485, Specificity: 0.7397\n",
      "Epoch 23/100, Loss: 0.0568, Acc: 0.7941, MCC: 0.5844, Sensitivity: 0.7784, Specificity: 0.8190\n",
      "Epoch 24/100, Loss: 0.0517, Acc: 0.7941, MCC: 0.5862, Sensitivity: 0.7745, Specificity: 0.8254\n",
      "Epoch 25/100, Loss: 0.0544, Acc: 0.7794, MCC: 0.5568, Sensitivity: 0.7605, Specificity: 0.8095\n",
      "Epoch 26/100, Loss: 0.0506, Acc: 0.7917, MCC: 0.5768, Sensitivity: 0.7824, Specificity: 0.8063\n",
      "Epoch 27/100, Loss: 0.0488, Acc: 0.8125, MCC: 0.6163, Sensitivity: 0.8084, Specificity: 0.8190\n",
      "Epoch 28/100, Loss: 0.0501, Acc: 0.8174, MCC: 0.6272, Sensitivity: 0.8104, Specificity: 0.8286\n",
      "Epoch 29/100, Loss: 0.0493, Acc: 0.8002, MCC: 0.5917, Sensitivity: 0.7964, Specificity: 0.8063\n",
      "Epoch 30/100, Loss: 0.0473, Acc: 0.8125, MCC: 0.6149, Sensitivity: 0.8124, Specificity: 0.8127\n",
      "Epoch 31/100, Loss: 0.0463, Acc: 0.8297, MCC: 0.6512, Sensitivity: 0.8244, Specificity: 0.8381\n",
      "Epoch 32/100, Loss: 0.0445, Acc: 0.8235, MCC: 0.6411, Sensitivity: 0.8124, Specificity: 0.8413\n",
      "Epoch 33/100, Loss: 0.0471, Acc: 0.8186, MCC: 0.6325, Sensitivity: 0.8044, Specificity: 0.8413\n",
      "Epoch 34/100, Loss: 0.0412, Acc: 0.8431, MCC: 0.6774, Sensitivity: 0.8403, Specificity: 0.8476\n",
      "Epoch 35/100, Loss: 0.0404, Acc: 0.8431, MCC: 0.6856, Sensitivity: 0.8184, Specificity: 0.8825\n",
      "Epoch 36/100, Loss: 0.0438, Acc: 0.8358, MCC: 0.6696, Sensitivity: 0.8144, Specificity: 0.8698\n",
      "Epoch 37/100, Loss: 0.0399, Acc: 0.8578, MCC: 0.7106, Sensitivity: 0.8443, Specificity: 0.8794\n",
      "Epoch 38/100, Loss: 0.0393, Acc: 0.8640, MCC: 0.7259, Sensitivity: 0.8423, Specificity: 0.8984\n",
      "Epoch 39/100, Loss: 0.0383, Acc: 0.8676, MCC: 0.7331, Sensitivity: 0.8463, Specificity: 0.9016\n",
      "Epoch 40/100, Loss: 0.0377, Acc: 0.8591, MCC: 0.7173, Sensitivity: 0.8343, Specificity: 0.8984\n",
      "Epoch 41/100, Loss: 0.0367, Acc: 0.8713, MCC: 0.7411, Sensitivity: 0.8483, Specificity: 0.9079\n",
      "Epoch 42/100, Loss: 0.0339, Acc: 0.8775, MCC: 0.7493, Sensitivity: 0.8663, Specificity: 0.8952\n",
      "Epoch 43/100, Loss: 0.0325, Acc: 0.8909, MCC: 0.7789, Sensitivity: 0.8723, Specificity: 0.9206\n",
      "Epoch 44/100, Loss: 0.0349, Acc: 0.8738, MCC: 0.7440, Sensitivity: 0.8563, Specificity: 0.9016\n",
      "Epoch 45/100, Loss: 0.0319, Acc: 0.8934, MCC: 0.7833, Sensitivity: 0.8762, Specificity: 0.9206\n",
      "Epoch 46/100, Loss: 0.0301, Acc: 0.9020, MCC: 0.8027, Sensitivity: 0.8782, Specificity: 0.9397\n",
      "Epoch 47/100, Loss: 0.0312, Acc: 0.8983, MCC: 0.7954, Sensitivity: 0.8743, Specificity: 0.9365\n",
      "Epoch 48/100, Loss: 0.0322, Acc: 0.9020, MCC: 0.8002, Sensitivity: 0.8862, Specificity: 0.9270\n",
      "Epoch 49/100, Loss: 0.0295, Acc: 0.9105, MCC: 0.8177, Sensitivity: 0.8942, Specificity: 0.9365\n",
      "Epoch 50/100, Loss: 0.0273, Acc: 0.9167, MCC: 0.8338, Sensitivity: 0.8882, Specificity: 0.9619\n",
      "Epoch 51/100, Loss: 0.0282, Acc: 0.9191, MCC: 0.8348, Sensitivity: 0.9042, Specificity: 0.9429\n",
      "Epoch 52/100, Loss: 0.0287, Acc: 0.9142, MCC: 0.8274, Sensitivity: 0.8902, Specificity: 0.9524\n",
      "Epoch 53/100, Loss: 0.0269, Acc: 0.9228, MCC: 0.8455, Sensitivity: 0.8962, Specificity: 0.9651\n",
      "Epoch 54/100, Loss: 0.0265, Acc: 0.9203, MCC: 0.8387, Sensitivity: 0.9002, Specificity: 0.9524\n",
      "Epoch 55/100, Loss: 0.0259, Acc: 0.9240, MCC: 0.8445, Sensitivity: 0.9102, Specificity: 0.9460\n",
      "Epoch 56/100, Loss: 0.0245, Acc: 0.9265, MCC: 0.8500, Sensitivity: 0.9102, Specificity: 0.9524\n",
      "Epoch 57/100, Loss: 0.0233, Acc: 0.9449, MCC: 0.8876, Sensitivity: 0.9281, Specificity: 0.9714\n",
      "Epoch 58/100, Loss: 0.0247, Acc: 0.9326, MCC: 0.8625, Sensitivity: 0.9162, Specificity: 0.9587\n",
      "Epoch 59/100, Loss: 0.0248, Acc: 0.9375, MCC: 0.8718, Sensitivity: 0.9242, Specificity: 0.9587\n",
      "Epoch 60/100, Loss: 0.0220, Acc: 0.9485, MCC: 0.8938, Sensitivity: 0.9381, Specificity: 0.9651\n",
      "Epoch 61/100, Loss: 0.0219, Acc: 0.9400, MCC: 0.8783, Sensitivity: 0.9202, Specificity: 0.9714\n",
      "Epoch 62/100, Loss: 0.0209, Acc: 0.9571, MCC: 0.9123, Sensitivity: 0.9421, Specificity: 0.9810\n",
      "Epoch 63/100, Loss: 0.0206, Acc: 0.9608, MCC: 0.9191, Sensitivity: 0.9501, Specificity: 0.9778\n",
      "Epoch 64/100, Loss: 0.0222, Acc: 0.9449, MCC: 0.8867, Sensitivity: 0.9321, Specificity: 0.9651\n",
      "Epoch 65/100, Loss: 0.0194, Acc: 0.9608, MCC: 0.9201, Sensitivity: 0.9441, Specificity: 0.9873\n",
      "Epoch 66/100, Loss: 0.0202, Acc: 0.9571, MCC: 0.9116, Sensitivity: 0.9461, Specificity: 0.9746\n",
      "Epoch 67/100, Loss: 0.0187, Acc: 0.9645, MCC: 0.9270, Sensitivity: 0.9521, Specificity: 0.9841\n",
      "Epoch 68/100, Loss: 0.0184, Acc: 0.9657, MCC: 0.9300, Sensitivity: 0.9501, Specificity: 0.9905\n",
      "Epoch 69/100, Loss: 0.0180, Acc: 0.9669, MCC: 0.9324, Sensitivity: 0.9521, Specificity: 0.9905\n",
      "Epoch 70/100, Loss: 0.0172, Acc: 0.9669, MCC: 0.9305, Sensitivity: 0.9681, Specificity: 0.9651\n",
      "Epoch 71/100, Loss: 0.0158, Acc: 0.9816, MCC: 0.9621, Sensitivity: 0.9721, Specificity: 0.9968\n",
      "Epoch 72/100, Loss: 0.0182, Acc: 0.9669, MCC: 0.9315, Sensitivity: 0.9581, Specificity: 0.9810\n",
      "Epoch 73/100, Loss: 0.0171, Acc: 0.9681, MCC: 0.9348, Sensitivity: 0.9541, Specificity: 0.9905\n",
      "Epoch 74/100, Loss: 0.0169, Acc: 0.9694, MCC: 0.9369, Sensitivity: 0.9581, Specificity: 0.9873\n",
      "Epoch 75/100, Loss: 0.0169, Acc: 0.9792, MCC: 0.9565, Sensitivity: 0.9741, Specificity: 0.9873\n",
      "Epoch 76/100, Loss: 0.0154, Acc: 0.9730, MCC: 0.9450, Sensitivity: 0.9581, Specificity: 0.9968\n",
      "Epoch 77/100, Loss: 0.0150, Acc: 0.9804, MCC: 0.9596, Sensitivity: 0.9701, Specificity: 0.9968\n",
      "Epoch 78/100, Loss: 0.0147, Acc: 0.9792, MCC: 0.9574, Sensitivity: 0.9661, Specificity: 1.0000\n",
      "Epoch 79/100, Loss: 0.0139, Acc: 0.9779, MCC: 0.9542, Sensitivity: 0.9701, Specificity: 0.9905\n",
      "Epoch 80/100, Loss: 0.0147, Acc: 0.9767, MCC: 0.9518, Sensitivity: 0.9681, Specificity: 0.9905\n",
      "Epoch 81/100, Loss: 0.0134, Acc: 0.9828, MCC: 0.9645, Sensitivity: 0.9741, Specificity: 0.9968\n",
      "Epoch 82/100, Loss: 0.0136, Acc: 0.9853, MCC: 0.9697, Sensitivity: 0.9760, Specificity: 1.0000\n",
      "Epoch 83/100, Loss: 0.0139, Acc: 0.9792, MCC: 0.9567, Sensitivity: 0.9721, Specificity: 0.9905\n",
      "Epoch 84/100, Loss: 0.0137, Acc: 0.9828, MCC: 0.9642, Sensitivity: 0.9780, Specificity: 0.9905\n",
      "Epoch 85/100, Loss: 0.0140, Acc: 0.9828, MCC: 0.9643, Sensitivity: 0.9760, Specificity: 0.9937\n",
      "Epoch 86/100, Loss: 0.0131, Acc: 0.9865, MCC: 0.9720, Sensitivity: 0.9800, Specificity: 0.9968\n",
      "Epoch 87/100, Loss: 0.0125, Acc: 0.9853, MCC: 0.9695, Sensitivity: 0.9780, Specificity: 0.9968\n",
      "Epoch 88/100, Loss: 0.0129, Acc: 0.9890, MCC: 0.9770, Sensitivity: 0.9840, Specificity: 0.9968\n",
      "Epoch 89/100, Loss: 0.0120, Acc: 0.9841, MCC: 0.9670, Sensitivity: 0.9760, Specificity: 0.9968\n",
      "Epoch 90/100, Loss: 0.0134, Acc: 0.9767, MCC: 0.9525, Sensitivity: 0.9621, Specificity: 1.0000\n",
      "Epoch 91/100, Loss: 0.0123, Acc: 0.9902, MCC: 0.9795, Sensitivity: 0.9860, Specificity: 0.9968\n",
      "Epoch 92/100, Loss: 0.0120, Acc: 0.9877, MCC: 0.9745, Sensitivity: 0.9820, Specificity: 0.9968\n",
      "Epoch 93/100, Loss: 0.0120, Acc: 0.9902, MCC: 0.9796, Sensitivity: 0.9840, Specificity: 1.0000\n",
      "Epoch 94/100, Loss: 0.0125, Acc: 0.9865, MCC: 0.9720, Sensitivity: 0.9800, Specificity: 0.9968\n",
      "Epoch 95/100, Loss: 0.0118, Acc: 0.9853, MCC: 0.9695, Sensitivity: 0.9780, Specificity: 0.9968\n",
      "Epoch 96/100, Loss: 0.0111, Acc: 0.9914, MCC: 0.9820, Sensitivity: 0.9900, Specificity: 0.9937\n",
      "Epoch 97/100, Loss: 0.0108, Acc: 0.9890, MCC: 0.9771, Sensitivity: 0.9820, Specificity: 1.0000\n",
      "Epoch 98/100, Loss: 0.0106, Acc: 0.9963, MCC: 0.9923, Sensitivity: 0.9960, Specificity: 0.9968\n",
      "Epoch 99/100, Loss: 0.0104, Acc: 0.9890, MCC: 0.9770, Sensitivity: 0.9840, Specificity: 0.9968\n",
      "Epoch 100/100, Loss: 0.0099, Acc: 0.9951, MCC: 0.9897, Sensitivity: 0.9960, Specificity: 0.9937\n",
      "accuracy: 0.7843137254901961\n",
      "balanced_accuracy: 0.7890632911392406\n",
      "MCC: 0.5654426031207677\n",
      "AUC: 0.865620253164557\n",
      "AP: 0.9098355104789038\n",
      "TN: 64\n",
      "FP: 15\n",
      "FN: 29\n",
      "TP: 96\n",
      "Sensitivity: 0.768\n",
      "Specificity: 0.810126582278481\n",
      "Precision: 0.8648648648648649\n",
      "Recall: 0.768\n",
      "F1 Score: 0.8135593220338984\n",
      "Youden Index: 0.5781265822784811\n",
      "<----------------Testing Complete---------------->\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 从 focalLoss.py 导入 FocalLoss_v2\n",
    "class FocalLoss_v2(nn.Module):\n",
    "    def __init__(self, num_class=2, gamma=2, alpha=None):\n",
    "        super(FocalLoss_v2, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.num_class = num_class\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        target = target.view(-1)\n",
    "        alpha = self.alpha[target.cpu().long()].to(logit.device)\n",
    "        logpt = -F.cross_entropy(logit, target, reduction='none')\n",
    "        pt = torch.exp(logpt)\n",
    "        focal_loss = -(alpha * (1 - pt) ** self.gamma) * logpt\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# 从 utility.py 导入 masked_softmax\n",
    "def create_src_lengths_mask(batch_size: int, src_lengths, max_src_len=None):\n",
    "    if max_src_len is None:\n",
    "        max_src_len = int(src_lengths.max())\n",
    "    src_indices = torch.arange(0, max_src_len).unsqueeze(0).type_as(src_lengths)\n",
    "    src_indices = src_indices.expand(batch_size, max_src_len)\n",
    "    src_lengths = src_lengths.unsqueeze(dim=1).expand(batch_size, max_src_len)\n",
    "    return (src_indices < src_lengths).int()\n",
    "\n",
    "def masked_softmax(scores, src_lengths, src_length_masking=True):\n",
    "    if src_length_masking:\n",
    "        bsz, src_len, max_src_len = scores.size()\n",
    "        src_mask = create_src_lengths_mask(bsz, src_lengths, max_src_len).to(scores.device)\n",
    "        src_mask = src_mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(src_mask == 0, -np.inf)\n",
    "    return F.softmax(scores.float(), dim=-1)\n",
    "\n",
    "# 定义 Contextual_Attention 类\n",
    "class Contextual_Attention(nn.Module):\n",
    "    def __init__(self, q_input_dim, v_input_dim=1024, qk_dim=1024, v_dim=1024):\n",
    "        super(Contextual_Attention, self).__init__()\n",
    "        self.cn3 = nn.Conv1d(q_input_dim, qk_dim, kernel_size=3, padding=1)\n",
    "        self.cn5 = nn.Conv1d(q_input_dim, qk_dim, kernel_size=5, padding=2)\n",
    "        self.k = nn.Linear(v_dim * 2 + q_input_dim, qk_dim)\n",
    "        self.q = nn.Linear(q_input_dim, qk_dim)\n",
    "        self.v = nn.Linear(v_input_dim, v_dim)\n",
    "        self._norm_fact = 1 / math.sqrt(qk_dim)\n",
    "\n",
    "    def forward(self, plm_embedding, evo_local, seqlengths):\n",
    "        Q = self.q(evo_local)\n",
    "        k3 = self.cn3(evo_local.permute(0, 2, 1))\n",
    "        k5 = self.cn5(evo_local.permute(0, 2, 1))\n",
    "        evo_local_concat = torch.cat((evo_local, k3.permute(0, 2, 1), k5.permute(0, 2, 1)), dim=2)\n",
    "        K = self.k(evo_local_concat)\n",
    "        V = self.v(plm_embedding)\n",
    "        atten_scores = torch.bmm(Q, K.permute(0, 2, 1)) * self._norm_fact\n",
    "        atten = masked_softmax(atten_scores, seqlengths)\n",
    "        output = torch.bmm(atten, V)\n",
    "        return output + V\n",
    "\n",
    "# coll_paddding 函数\n",
    "def coll_paddding(batch_traindata):\n",
    "    batch_traindata.sort(key=lambda data: len(data[0]), reverse=True)\n",
    "    feature0 = []\n",
    "    f0agv = []\n",
    "    feature_fusion = []\n",
    "    train_y = []\n",
    "    for data in batch_traindata:\n",
    "        feature0.append(data[0])\n",
    "        f0agv.append(data[1])\n",
    "        feature_fusion.append(data[2])\n",
    "        train_y.append(data[3])\n",
    "    data_length = [len(data) for data in feature0]\n",
    "    feature0 = torch.nn.utils.rnn.pad_sequence(feature0, batch_first=True, padding_value=0)\n",
    "    f0agv = torch.nn.utils.rnn.pad_sequence(f0agv, batch_first=True, padding_value=0)\n",
    "    feature_fusion = torch.nn.utils.rnn.pad_sequence(feature_fusion, batch_first=True, padding_value=0)\n",
    "    train_y = torch.nn.utils.rnn.pad_sequence(train_y, batch_first=True, padding_value=0)\n",
    "    return feature0, f0agv, feature_fusion, train_y, torch.tensor(data_length)\n",
    "\n",
    "# 数据集类\n",
    "class BioinformaticsDataset(Dataset):\n",
    "    def __init__(self, X_prot, X_feature_fusion):\n",
    "        self.X_prot = X_prot\n",
    "        self.X_feature_fusion = X_feature_fusion\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename_prot = self.X_prot[index]\n",
    "        df_prot = pd.read_csv(filename_prot)\n",
    "        prot = df_prot.iloc[:, 1:].values\n",
    "        if prot.dtype == object:\n",
    "            prot = prot.astype(float)\n",
    "        prot = torch.tensor(prot, dtype=torch.float)\n",
    "        agv = torch.mean(prot, dim=0)\n",
    "        agv = agv.repeat(prot.shape[0], 1)\n",
    "        feature_fusion = prot  # 如果没有特征融合数据，可以使用prot\n",
    "        label = df_prot.iloc[:, 0].values\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return prot, agv, feature_fusion, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_prot)\n",
    "\n",
    "# 模型定义\n",
    "class DeepAIPModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepAIPModule, self).__init__()\n",
    "        self.ca = Contextual_Attention(q_input_dim=1024, v_input_dim=1024)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.protcnn1 = nn.Conv1d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.protcnn2 = nn.Conv1d(512, 256, kernel_size=3, padding=1)\n",
    "        self.protcnn3 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(32)\n",
    "    def forward(self, prot0, f0agv, evo, data_length):\n",
    "        #evosa = self.ca(prot0, evo, data_length)\n",
    "        #prot = torch.cat((prot0, f0agv, evosa), dim=2)\n",
    "        prot = self.protcnn1(prot0.permute(0, 2, 1))\n",
    "        prot = self.relu(prot)\n",
    "        prot = self.protcnn2(prot)\n",
    "        prot = self.relu(prot)\n",
    "        prot = self.protcnn3(prot)\n",
    "        prot = self.relu(prot)\n",
    "        x = self.fc2(prot.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)  # 调整维度以适应 BatchNorm1d 的输入\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # 恢复原始的维度顺序\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = x.permute(0, 2, 1)  # 调整维度以适应 BatchNorm1d 的输入\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # 恢复原始的维度顺序\n",
    "        x = self.drop(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# 训练函数\n",
    "def train():\n",
    "    train_set = BioinformaticsDataset(prot_train, fusion_train)\n",
    "    model = DeepAIPModule()\n",
    "    epochs = 100\n",
    "    model = model.to(device)\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=256, shuffle=True, num_workers=4,\n",
    "                              collate_fn=coll_paddding)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    per_cls_weights = torch.FloatTensor([0.6, 0.4]).to(device)\n",
    "    fcloss = FocalLoss_v2(alpha=per_cls_weights, gamma=2)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_train = 0.0\n",
    "        nb_train = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for prot_x, f0agv, evo_x, data_y, length in train_loader:\n",
    "            prot_x = prot_x.to(device)\n",
    "            f0agv = f0agv.to(device)\n",
    "            evo_x = evo_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "            length = length.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(prot_x, f0agv, evo_x, length)\n",
    "            y_pred_packed = torch.nn.utils.rnn.pack_padded_sequence(y_pred, length.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            data_y_packed = torch.nn.utils.rnn.pack_padded_sequence(data_y, length.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            loss = fcloss(y_pred_packed.data, data_y_packed.data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss_train += loss.item()\n",
    "            nb_train += 1\n",
    "            # 收集预测结果和真实标签用于计算指标\n",
    "            y_pred_labels = torch.argmax(y_pred_packed.data, dim=1).cpu()\n",
    "            all_preds.extend(y_pred_labels.numpy())\n",
    "            all_labels.extend(data_y_packed.data.cpu().numpy())\n",
    "        epoch_loss_avg = epoch_loss_train / nb_train\n",
    "        # 计算指标\n",
    "        acc = metrics.accuracy_score(all_labels, all_preds)\n",
    "        mcc = metrics.matthews_corrcoef(all_labels, all_preds)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        # 输出训练指标\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss_avg:.4f}, Acc: {acc:.4f}, MCC: {mcc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "    # 训练结束后对测试集进行评估\n",
    "    test(model)\n",
    "\n",
    "# 修改后的测试函数\n",
    "def test(model):\n",
    "    # Dataset and DataLoader Initialization\n",
    "    test_set = BioinformaticsDataset(prot_test, fusion_test)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=256, num_workers=4, collate_fn=coll_paddding)\n",
    "    model.eval()\n",
    "    arr_probs = []\n",
    "    arr_labels = []\n",
    "    arr_labels_hyps = []\n",
    "    with torch.no_grad():\n",
    "        for prot_x, f0agv, evo_x, data_y, length in test_loader:\n",
    "            prot_x = prot_x.to(device)\n",
    "            f0agv = f0agv.to(device)\n",
    "            evo_x = evo_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "            length = length.to(device)\n",
    "            y_pred = model(prot_x, f0agv, evo_x, length)\n",
    "            y_pred_packed = torch.nn.utils.rnn.pack_padded_sequence(y_pred, length.cpu(), batch_first=True)\n",
    "            y_pred_data = y_pred_packed.data\n",
    "            y_pred_softmax = torch.nn.functional.softmax(y_pred_data, dim=1)\n",
    "            arr_probs.extend(y_pred_softmax[:, 1].cpu().numpy())\n",
    "            y_pred_labels = torch.argmax(y_pred_softmax, dim=1).cpu()\n",
    "            data_y_packed = torch.nn.utils.rnn.pack_padded_sequence(data_y, length.cpu(), batch_first=True)\n",
    "            arr_labels.extend(data_y_packed.data.cpu())\n",
    "            arr_labels_hyps.extend(y_pred_labels.cpu())\n",
    "    auc = metrics.roc_auc_score(arr_labels, arr_probs)\n",
    "    acc = metrics.accuracy_score(arr_labels, arr_labels_hyps)\n",
    "    mcc = metrics.matthews_corrcoef(arr_labels, arr_labels_hyps)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(arr_labels, arr_labels_hyps, labels=[0,1]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1score = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    youden = sensitivity + specificity - 1\n",
    "    # Results Storage and Output\n",
    "    metrics_dict = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': metrics.balanced_accuracy_score(arr_labels, arr_labels_hyps),\n",
    "        'MCC': mcc,\n",
    "        'AUC': auc,\n",
    "        'AP': metrics.average_precision_score(arr_labels, arr_probs),\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TP': tp,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1score,\n",
    "        'Youden Index': youden\n",
    "    }\n",
    "    for key, value in metrics_dict.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "    print('<----------------Testing Complete---------------->')\n",
    "    return acc, mcc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 检查 CUDA 是否可用\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        torch.cuda.set_device(0)\n",
    "    print(\"Use device:\", device)\n",
    "    # 数据集文件名\n",
    "    fusion_train = ['/root/ACE/train_data.csv']\n",
    "    prot_train = ['/root/ACE/train_data.csv']\n",
    "    fusion_test = ['/root/ACE/test_data.csv']\n",
    "    prot_test = ['/root/ACE/test_data.csv']\n",
    "    # 开始训练\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "po",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
